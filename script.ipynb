{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXVD2Xc_p1h3",
        "outputId": "363781d6-1fdc-450e-ffd2-5bd97e73e39b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'exact_cognates': [('brat', 'brother'), ('cierń', 'thorn'), ('gęś', 'goose'), ('kamień', 'heaven'), ('mech', 'moss'), ('miód', 'mead'), ('mysz', 'mouse'), ('noc', 'night'), ('nos', 'nose'), ('oko', 'eye'), ('pierdzieć', 'fart'), ('pięść', 'fist'), ('ramię', 'arm'), ('siedem', 'seven'), ('trzoda', 'herd'), ('ucho', 'ear'), ('wełna', 'wool'), ('woda', 'water'), ('wymię', 'udder'), ('ząb', 'comb'), ('ziarno', 'corn'), ('żywy', 'quick'), ('bosy', 'bare'), ('bóbr', 'beaver'), ('brać', 'bear'), ('brat', 'brother'), ('broda', 'beard'), ('brzoza', 'birch'), ('cały', 'whole'), ('dwa', 'two'), ('gęś', 'goose'), ('gnieść', 'knead'), ('jutro', 'east'), ('kłoda', 'holt'), ('kuć', 'hew'), ('łub', 'leaf'), ('mały', 'small'), ('mazać', 'make'), ('mech', 'moss'), ('miód', 'mead'), ('mysz', 'mouse'), ('noc', 'night'), ('nos', 'nose'), ('nowy', 'new'), ('oko', 'eye'), ('osiem', 'eight'), ('pełny', 'full'), ('piana', 'foam'), ('piąty', 'fifth'), ('pięść', 'fist'), ('płosa', 'fallow'), ('płowy', 'fallow'), ('reż', 'rye'), ('rudy', 'red'), ('sarna', 'horn'), ('sen', 'sweven'), ('siedzieć', 'sit'), ('siostra', 'sister'), ('słoma', 'haulm'), ('spać', 'sweve'), ('srom', 'harm'), ('strumień', 'stream'), ('suchy', 'sear'), ('swarzyć', 'swear'), ('syn', 'son'), ('śnieg', 'snow'), ('świnia', 'swine'), ('tajać', 'thaw'), ('trzoda', 'herd'), ('trzy', 'three'), ('ty', 'thou'), ('tysiąc', 'thousand'), ('wabić', 'weep'), ('wdowa', 'widow'), ('wełna', 'wool'), ('wieczór', 'west'), ('wilk', 'wolf'), ('władać', 'wield'), ('woda', 'water'), ('wosk', 'wax'), ('ząb', 'comb'), ('ziarno', 'corn'), ('złoto', 'gold'), ('znać', 'know'), ('żarna', 'quern'), ('żona', 'quean'), ('żywy', 'quick')], 'base_cognates': [('bóść', 'bed'), ('dziąsło', 'tooth'), ('glina', 'clay'), ('imię', 'name'), ('iskać', 'ask'), ('lizać', 'lick'), ('prosię', 'farrow'), ('sarna', 'horn'), ('szerszeń', 'hornet'), ('bóść', 'bed'), ('córka', 'daughter'), ('czesać', 'hair'), ('dno', 'deep'), ('dziąsło', 'tooth'), ('dziesięć', 'ten'), ('glina', 'clay'), ('gładki', 'glad'), ('goły', 'callow'), ('gród', 'gird'), ('imię', 'name'), ('iskać', 'ask'), ('jabłko', 'apple'), ('język', 'tongue'), ('lizać', 'lick'), ('łapa', 'glove'), ('łgać', 'lie'), ('matka', 'mother'), ('mąż', 'man'), ('owca', 'ewe'), ('pieszy', 'foot'), ('pławić', 'flow'), ('prosię', 'farrow'), ('rżysko', 'rye'), ('słońce', 'sun'), ('sto', 'hundred'), ('suka', 'hound'), ('ściana', 'stone'), ('wełna', 'wool'), ('wola', 'will'), ('żebro', 'rib')], 'base_cognates_foreign': [('serce', 'heart')], 'cognate_roots': [('mleko', 'milk'), ('alfabet', 'alphabet'), ('armia', 'army'), ('buk', 'beech'), ('chleb', 'loaf'), ('dekret', 'decree'), ('integracja', 'integration'), ('intonacja', 'intonation'), ('kardiologia', 'cardiology'), ('kardynał', 'cardinal'), ('komunia', 'communion'), ('kot', 'cat'), ('kronika', 'chronicle'), ('kryształ', 'crystal'), ('ksiądz', 'king'), ('kupić', 'cheap'), ('kusić', 'choose'), ('manuskrypt', 'manuscript'), ('maszyna', 'machine'), ('muzyka', 'music'), ('opera', 'opera'), ('oś', 'axis'), ('pielgrzym', 'pilgrim'), ('pieniądz', 'penny'), ('piła', 'file'), ('pług', 'plough'), ('pop', 'pope'), ('post', 'fast'), ('problem', 'problem'), ('pułk', 'folk'), ('teologia', 'theology'), ('teza', 'thesis'), ('toaleta', 'toilet'), ('tragedia', 'tragedy'), ('tygrys', 'tiger'), ('tymianek', 'thyme'), ('tytuł', 'title'), ('werdykt', 'verdict')], 'exact_cognates_foreign': [('łeż', 'lie'), ('moc', 'might'), ('łeż', 'lie'), ('moc', 'might'), ('łeż', 'lie'), ('moc', 'might')], 'cognates_derivatives': [('grób', 'grave'), ('koło', 'wheel'), ('nić', 'needle'), ('żal', 'qualm'), ('żal', 'quell'), ('koło', 'wheel'), ('nić', 'needle'), ('przyjaciel', 'friend'), ('wiatr', 'wind'), ('żal', 'qualm'), ('grób', 'grave'), ('nić', 'needle'), ('siodło', 'saddle'), ('wiatr', 'wind'), ('żal', 'qualm'), ('żal', 'quell')], 'loanwords': [('gość', 'guest'), ('brzeg', 'berg'), ('gość', 'guest')]}\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def classify_text(file):\n",
        "\n",
        "    pattern1 = r\"\\bP\\s+[a-ząćęłńóśźż]+\\s*=\\s*E\\s+[a-ząćęłńóśźż]+\\b\"  #exact_cognates\n",
        "    pattern2 = r\"\\bP\\s+[a-ząćęłńóśźż]+\\s*&\\s*E\\s+[a-ząćęłńóśźż]+\\b\"   #base_cognates\n",
        "    pattern3 = r\"\\bP\\s+[a-ząćęłńóśźż]+\\s*&\\/\\s*E\\s+[a-ząćęłńóśźż]+\\b\"  #base_cognates_foreign\n",
        "    pattern4 = r\"\\bP\\s+[a-ząćęłńóśźż]+\\s*//\\s*E\\s+[a-ząćęłńóśźż]+\\b\"   #loanwords\n",
        "    pattern5 = r\"\\bP\\s+[a-ząćęłńóśźż]+\\s*\\^\\s*E\\s+[a-ząćęłńóśźż]+\\b\"    #in_cognate_roots\n",
        "    pattern6 = r\"\\bP\\s+[a-ząćęłńóśźż]+\\s*=\\/\\s*E\\s+[a-ząćęłńóśźż]+\\b\" #exact_cognate_foreign\n",
        "    pattern7 = r\"\\bP\\s+[a-ząćęłńóśźż]+\\s*=\\^\\s*E\\s+[a-ząćęłńóśźż]+\\b\" #cognates_derivatives\n",
        "\n",
        "\n",
        "    exact_cognates = []\n",
        "    base_cognates = []\n",
        "    base_cognates_foreign = []\n",
        "    in_cognate_roots = []\n",
        "    cognates_derivatives = []\n",
        "    loanwords = []\n",
        "    exact_cognate_foreign = []\n",
        "\n",
        "\n",
        "    with open(file, \"r\") as f:\n",
        "        content = f.readlines()\n",
        "\n",
        "   #Categorisation\n",
        "    for line in content:\n",
        "        if re.search(pattern1, line):\n",
        "          exact_cognates.append(line.strip())\n",
        "        elif re.search(pattern2, line):\n",
        "          base_cognates.append(line.strip())\n",
        "        elif re.search(pattern3, line):\n",
        "          base_cognates_foreign.append(line.strip())\n",
        "        elif re.search(pattern4, line):\n",
        "          in_cognate_roots.append(line.strip())\n",
        "        elif re.search(pattern5, line):\n",
        "          cognates_derivatives.append(line.strip())\n",
        "        elif re.search(pattern6, line):\n",
        "          loanwords.append(line.strip())\n",
        "        elif re.search(pattern7, line):\n",
        "          exact_cognate_foreign.append(line.strip())\n",
        "\n",
        "\n",
        "  #Clean and tuple\n",
        "\n",
        "    pattern_polish = r\"\\bP\\s+([a-ząćęłńóśźż]+)\"\n",
        "    pattern_english = r\"\\bE\\s+([a-zA-Z]+)\"\n",
        "\n",
        "    exact_cognates_tuples = []\n",
        "    base_cognates_tuples = []\n",
        "    base_cognates_foreign_tuples = []\n",
        "    in_cognate_roots_tuples = []\n",
        "    cognates_derivatives_tuples = []\n",
        "    loanwords_tuples = []\n",
        "    exact_cognate_foreign_tuples = []\n",
        "\n",
        "  #1exact_cognates\n",
        "    for string in exact_cognates:\n",
        "        polish_match = re.search(pattern_polish, string)\n",
        "        english_match = re.search(pattern_english, string)\n",
        "\n",
        "        if polish_match and english_match:\n",
        "            polish_word = polish_match.group(0)\n",
        "            english_word = english_match.group(0)\n",
        "\n",
        "            exact_cognates_tuples.append((polish_word[2:], english_word[2:]))\n",
        "  #2base_cognates\n",
        "    for string in base_cognates:\n",
        "        polish_match = re.search(pattern_polish, string)\n",
        "        english_match = re.search(pattern_english, string)\n",
        "\n",
        "        if polish_match and english_match:\n",
        "            polish_word = polish_match.group(0)\n",
        "            english_word = english_match.group(0)\n",
        "\n",
        "            base_cognates_tuples.append((polish_word[2:], english_word[2:]))\n",
        "  #3base_cognates_foreign\n",
        "    for string in base_cognates_foreign:\n",
        "        polish_match = re.search(pattern_polish, string)\n",
        "        english_match = re.search(pattern_english, string)\n",
        "\n",
        "        if polish_match and english_match:\n",
        "            polish_word = polish_match.group(0)\n",
        "            english_word = english_match.group(0)\n",
        "\n",
        "            base_cognates_foreign_tuples.append((polish_word[2:], english_word[2:]))\n",
        "\n",
        "   #4loanwards\n",
        "    for string in loanwords:\n",
        "        polish_match = re.search(pattern_polish, string)\n",
        "        english_match = re.search(pattern_english, string)\n",
        "\n",
        "        if polish_match and english_match:\n",
        "            polish_word = polish_match.group(0)\n",
        "            english_word = english_match.group(0)\n",
        "\n",
        "            loanwords_tuples.append((polish_word[2:], english_word[2:]))\n",
        "\n",
        "  #5in_cognate_roots\n",
        "    for string in in_cognate_roots:\n",
        "        polish_match = re.search(pattern_polish, string)\n",
        "        english_match = re.search(pattern_english, string)\n",
        "\n",
        "        if polish_match and english_match:\n",
        "            polish_word = polish_match.group(0)\n",
        "            english_word = english_match.group(0)\n",
        "\n",
        "            in_cognate_roots_tuples.append((polish_word[2:], english_word[2:]))\n",
        "\n",
        "  #6exact_cognate_foreign\n",
        "    for string in exact_cognate_foreign:\n",
        "        polish_match = re.search(pattern_polish, string)\n",
        "        english_match = re.search(pattern_english, string)\n",
        "\n",
        "        if polish_match and english_match:\n",
        "            polish_word = polish_match.group(0)\n",
        "            english_word = english_match.group(0)\n",
        "\n",
        "            exact_cognate_foreign_tuples.append((polish_word[2:], english_word[2:]))\n",
        "\n",
        "  #7cognates_derivatives\n",
        "    for string in cognates_derivatives:\n",
        "        polish_match = re.search(pattern_polish, string)\n",
        "        english_match = re.search(pattern_english, string)\n",
        "\n",
        "        if polish_match and english_match:\n",
        "            polish_word = polish_match.group(0)\n",
        "            english_word = english_match.group(0)\n",
        "\n",
        "            cognates_derivatives_tuples.append((polish_word[2:], english_word[2:]))\n",
        "\n",
        "\n",
        "\n",
        "    # Print the list of cognates found\n",
        "    data = {\n",
        "    \"exact_cognates\": exact_cognates_tuples,\n",
        "    \"base_cognates\": base_cognates_tuples,\n",
        "    \"base_cognates_foreign\": base_cognates_foreign_tuples,\n",
        "    \"cognate_roots\": in_cognate_roots_tuples,\n",
        "    \"exact_cognates_foreign\": exact_cognate_foreign_tuples,\n",
        "    \"cognates_derivatives\": cognates_derivatives_tuples,\n",
        "    \"loanwords\": loanwords_tuples,\n",
        "    }\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "cognates_data = classify_text(\"cognates.txt\")\n",
        "\n",
        "print(cognates_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\\\n",
        "import nltk\n",
        "from nltk.chat.util import Chat, reflections\n",
        "import spacy\n",
        "import requests\n",
        "from datetime import date\n",
        "import re\n",
        "import random\n",
        "\n",
        "\n",
        "# Download spaCy model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    from spacy.cli import download\n",
        "    download(\"en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Patterns and responses for simple queries\n",
        "pairs = [\n",
        "    [r'hi|.*hello.*', ['Hello! 👋\\n', 'Hi there! 👋\\n']],\n",
        "    [r'.*how are you.*|.*what\\'s up.*', ['I am fantastic 🚀🚀, thank you! \\n Need any help with Polish/English cognates today?\\n']],\n",
        "    [r'.*how is your day.*', ['My day is AMAZING thanks to you 🫵, thank you for asking! \\n Now, lets get cracking with these cognates!💪💪\\n']],\n",
        "    [r'.*your name.*', [\"I don't have name yet 😿, please give your suggestions to Ania and Weronika.\\n\"]],\n",
        "    [r'.*help.*', ['Not to worry! You gott this!💪 \\n Now, tell me how I can help. ☺️ \\n', \"I know historical linguistics is difficult, but you're not alone! 🤝 \\n So, what's your question? \\n\", \"Don't worry, we can do it 💗together💗! \\n How can I help?\\n\"]],\n",
        "    [r'.*what can you do.*', ['I can help you find out if and how two words — one in English and one in Polish — are related, \\n or if a specific word is connected to another in our database. \\n Oh, and I also know a few jokes 😉. \\n']],\n",
        "    [r'.*what is the date.*', [f'Today is {str(date.today())}. Go and enjoy it 🥳\\n']],\n",
        "    [r'.*tell me a joke.*', ['Why don\\'t skeletons💀💀 fight each other? \\n They don\\'t have the guts! \\n 😂😂😂😂😂\\n', 'What do you call fake spaghetti🍝? \\n An impasta! \\n 😂😂😂😂\\n']],\n",
        "    [r'.*what are cognates.*', ['🔥SUCH a good question!🔥 \\nTrask (2000: 62) defines them as one of two or more words or morphemes which are directly descended from a single common ancestor \\nof the languages in which the words or morphemes are found, with no borrowing.\\n']],\n",
        "    [r'.*researchers.*', ['Mikołaj Rychło 👨‍🎓 and his loyal disciple Anna Potrykus 👩‍🎓\\n']],\n",
        "    [r'.*pieszy and foot.*', ['Suprise!🎉 Yes, they are! For more information see Potrykus 2023. \\n(shameless plug🔌)\\n']],\n",
        "    [r'.*best programmers.*', ['That\\'s easy! Of course, Weronika Paluch🥷 and Ania Potrykus🥷!\\n']],\n",
        "    [r'.*what are borrowings.*', ['📚In linguistics, lexical borrowing is the process\\n by which a word from one language is adapted for use in another.\\n The word that is borrowed is called \\n1️⃣ a borrowing, \\n2️⃣ a borrowed word, or \\n3️⃣ a loanword.\\n']],\n",
        "    [r'.*research\\b.*|.*article.*|.*paper.*', ['You got it! Have a look at 👉 Rychło, Mikołaj. (2019). Contrasting Cognates in Modern Languages from a Diachronic Perspective.\\n']],\n",
        "    [r'.*why.*', ['Why not?🤡\\n', \"That's so obvious I thought even you'd have already figured it out.🤡🤡\\n\"]],\n",
        "    [r'.*i think.*', ['You actually think?🫨 I\\'d never have guessed...\\n💀\\n', 'I wouldn\\'t think too hard if I were you.😪\\n']],\n",
        "    [r'.*shut up.*', ['Make me.🥱\\n', 'Getting angry at a feeble NLP assignment? Somebody\\'s losing it.🫠🫠\\n', 'Say that again, I dare you.😈😈\\n']],\n",
        "    [r'.*you\\'re rude.*', ['Sorry! I\\'m just kidding.🥴\\n']],\n",
        "    [r'.*exact cognates\\b.*', ['📚Cognates that have developed from the same proto-form with no morphological changes (Exact Cognates Sensu Stricto)\\n or with stem-formative adjustments (Exact Cognates Sensu Lato)\\n']],\n",
        "    [r'.*base cognates\\b.*', ['📚Cognates in which one of the compared languages exhibits a derivative whose base occurs in the cognate language\\n']],\n",
        "    [r'.*base cognates foreign.*', ['📚Cognates in which one of the compared languages exhibits a derivative whose base occurs in the cognate language,\\n one of the comparanda displays a foreign trait\\n']],\n",
        "    [r'.*cognate roots.*', ['📚Not cognates, but independent derivatives in both languages with cognate roots\\n']],\n",
        "    [r'.*cognates derivatives.*', ['📚Cognates which can simultaneously be considered independent derivatives\\n']],\n",
        "    [r'.*loanwords.*', ['📚Words related by borrowing (direct or indirect, also borrowed independently from a third source)\\n']],\n",
        "    [r'.*exact cognates foreign.*', ['📚Exact cognates with a foreign trait\\n']],\n",
        "    [r'.*thank you.*|.*thanks.*', ['You got it!🤗', 'You\\'re welcome!\\n🫡🫡']],\n",
        "    [r'.*tell me a fun fact.*',[\"The oldest living land animal on earth \\nis a 192-year-old 🐢tortoise🐢  named Jonathan.\\n\",\"Allodoxaphobia is the fear of other people’s opinions.\\n#relatble\\n\", \"Penicillin was first called “mold juice.”🤢🤢\\n\"]],\n",
        "    ]\n",
        "\n",
        "# Function to check if two words are cognates\n",
        "def are_cognates(word1, word2):\n",
        "    for key, cognates in cognates_data.items():\n",
        "        if (word1, word2) in cognates or (word2, word1) in cognates:\n",
        "            return f\"Yes, '{word1}' and '{word2}' are related. Type of relation: {key.replace('_', ' ')}.\"\n",
        "    return f\"Idk🤷, ask Rychło if '{word1}' and '{word2}' are related\"\n",
        "\n",
        "# Function to find a cognate of a provided word\n",
        "def find_related_word(word):\n",
        "    for key, cognates in cognates_data.items():\n",
        "        for pair in cognates:\n",
        "            if word in pair:\n",
        "                related_word = pair[0] if pair[1] == word else pair[1]\n",
        "                return f\"Yes, '{word}' is related to '{related_word}'. Type of relation: {key.replace('_', ' ')}.\\n\"\n",
        "    return f\"Idk🤷, ask Rychło if '{word}' is related to anything.\"\n",
        "\n",
        "# Function to provide examples of a given cognate category\n",
        "def provide_examples(category):\n",
        "    if category in cognates_data and cognates_data[category]:\n",
        "        examples = random.sample(cognates_data[category], min(3, len(cognates_data[category])))\n",
        "        formatted_examples = \", \".join([f\"'{tup[0]}' - '{tup[1]}'\" for tup in examples])\n",
        "        return f\"Here are some examples of {category.replace('_', ' ')}: {formatted_examples}.\\n\"\n",
        "    return f\"Sorry, I don't have examples for {category.replace('_', ' ')}.\\n\"\n",
        "\n",
        "# Function to extend chatbot with cognates knowledge\n",
        "def cognate_chatbot_response(user_input):\n",
        "    match = re.match(r\".*are ([a-ząćęłńóśźż]+) and ([a-zA-Z]+) related.*\", user_input)\n",
        "    if match:\n",
        "        word1, word2 = match.groups()\n",
        "        return are_cognates(word1, word2)\n",
        "\n",
        "    # Check if one word is related to another\n",
        "    match_related = re.match(r\".*is ([a-ząćęłńóśźż]+) related to anything.*\", user_input)\n",
        "    if match_related:\n",
        "        word = match_related.group(1)\n",
        "        return find_related_word(word)\n",
        "\n",
        "    match_examples = re.match(r\".*examples of (exact cognates|base cognates|base cognates foreign|cognate roots|cognates derivatives|loanwords|exact cognates foreign).*\", user_input)\n",
        "    if match_examples:\n",
        "        category = match_examples.group(1).replace(\" \", \"_\")  # Convert to dictionary key format\n",
        "        return provide_examples(category)\n",
        "\n",
        "    return None\n",
        "\n",
        "# Main chatbot function\n",
        "def chatbot():\n",
        "    print(\"⏳Chatbot: Hi! I'm a chatbot who can help you with ✨historical linguistics✨ and relations between words. Supercool🤠, right? \\n When you get tired of talking to me, type 'bye' to end the chat🫡.\\n\")\n",
        "\n",
        "    chat = Chat(pairs, reflections)  # Initialize nltk's rule-based chatbot\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"⭐You: \").lower()\n",
        "        print()\n",
        "\n",
        "        # Exit condition\n",
        "        if user_input == \"bye\":\n",
        "            print(\"⏳Chatbot: Glad I could help! See you next time. \\n By Anna Potrykus & Weronika Paluch 👭🏻 \\n (and Prof. Rychło 👨‍🏫)\")\n",
        "            break\n",
        "\n",
        "        # Cognate-based response\n",
        "        cognate_response = cognate_chatbot_response(user_input)\n",
        "        if cognate_response:\n",
        "            print(f\"⏳Chatbot: {cognate_response}\")\n",
        "            continue\n",
        "\n",
        "        # Pattern-based response\n",
        "        response = chat.respond(user_input)\n",
        "        if response:\n",
        "            print(f\"⏳Chatbot: {response}\")\n",
        "            continue\n",
        "        else:\n",
        "            print(f\"⏳Chatbot: Sorry, but I don't understand.\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    chatbot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "5SeIsyxgp6Sd",
        "outputId": "f2e82b66-c7be-4d79-9332-8720899d6c2d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏳Chatbot: Hi! I'm a chatbot who can help you with ✨historical linguistics✨ and relations between words. Supercool🤠, right? \n",
            " When you get tired of talking to me, type 'bye' to end the chat🫡.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-2b6b5c957413>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0mchatbot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-2b6b5c957413>\u001b[0m in \u001b[0;36mchatbot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"⭐You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}